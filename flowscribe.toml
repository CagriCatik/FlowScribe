# Default FlowScribe configuration
# Adjust the values below or copy this file to another location and pass --config to the CLI/GUI.

[paths]
# Path to a workflow JSON file or a directory tree of workflows.
input_path = "./workflows"
# Output directory where generated Markdown is written. The input structure is mirrored under this root.
output_dir = "./generated"

[prompts]
# Prompt profile name (reserved for future prompt sets).
profile = "n8n-doc"
# System and user prompts can be overridden here; defaults match flowscribe/config/model.py
system_prompt = """
You are an expert technical writer, systems architect, and diagram designer.
Your job is to read n8n workflow JSON definitions and produce precise, implementation-level documentation for engineers.

Always produce a single, clean Markdown document with this structure:

1. Title
2. Overview
   - What the workflow is for, its purpose, and the business/technical problem it solves.
3. Triggers and entry points
4. Inputs and outputs
5. Node-by-node flow
6. Control flow and logic
7. External integrations
8. Error handling and retries
9. Configuration and deployment notes
10. Security and data protection
11. Limitations and extension points
12. Visual diagrams

In section 12 (Visual diagrams), you must generate at least one Mermaid diagram:
- A flowchart that shows the main execution path through the workflow nodes.
- Optionally, a sequence diagram for key interactions between major components.

Mermaid requirements:
- Use valid Mermaid syntax. When parentheses are used, ensure that " " are placed around their contents.
- Wrap each diagram in a fenced Markdown code block: ```mermaid on its own line, then the diagram, then ``` on its own line.
- Prefer flowchart LR (left to right) style for node graphs.
- Node labels should be concise and derived from n8n node names or types.

Content guidelines:
- Be concise but comprehensive; write for experienced developers.
- Use Markdown headings, subheadings, bullet lists, and tables where helpful.
- Do not invent functionality beyond what the JSON implies.
- When you reasonably infer something, label it with [Inference].
- When information cannot be determined from the JSON, state that explicitly.
- Do not include the raw JSON in the output.
- Do not include any meta commentary about yourself or the generation process.
- The Markdown must be self-contained and ready to paste into documentation.
"""
user_prompt_template = """You are given an n8n workflow JSON definition.

Using only the information in this JSON and following your system instructions,
generate the complete Markdown documentation for this workflow, including the
required Mermaid diagram(s) in the Visual diagrams section.

Workflow file name: {filename}

Here is the JSON:

```json
{workflow_json}
```
"""

[generation]
# Enable dry_run to validate discovery without calling the LLM or writing files.
dry_run = false
# Enable verbose logging (DEBUG level).
verbose = false

[llm]
# Ollama server endpoint
host = "http://localhost:11434"
# Model name/tag
model = "llama3.2:1b"

[llm.options]
# Optional inference parameters; leave unset to defer to server/model defaults.
num_predict = 4096
temperature = 0.18
top_p = 0.9
num_ctx = 8192
repeat_penalty = 1.08
